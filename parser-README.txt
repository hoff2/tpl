Charles Hoffman
Translation of Programming Languages
Fall 2007

Project 2: Parser

The design for my parser actually started out as a design for a program to generate first and follow sets and possibly also a parse table.  I got it into my head that rather than the approach of iterating through the grammar's symbols (either in bottom-up order for first sets and top-down order for follow sets, or in arbitrary order repetitively until no sets receive new symbols, as in Louden), the process could probably be adapted to something in a more functional style: for example, computation of first sets has a nice inductive base case -- the first set of a terminal is the set containing just that terminal -- and inductive steps based on the grammar, and that therefore computing the sets through a recursive process would be implicitly bottom-up.  I came up with a simple list-based representation for the grammar and went to work on it.

It became slightly more complicated for follow sets; At first, the follow-set function computed some sets correctly, but went into infinite loops on others that crashed the Lisp interpreter.  At first, I took note of the fact that the algorithm for computing the follow set of some symbol a, often includes adding to it the follow set of a (because a appears on both sides of the rule), and that this unchecked recursion was causing the crashes, so I had the function check for this case and skip that step if the left side of the rule was the same symbol being currently worked on.  But some symbols still crashed, and I quickly realized that the loop could involve multiple symbols -- for example, the follow set of a includes the follow set of b, which includes the follow set of a again -- and that in fact there could be an arbitrary number of symbols involved in such a situation.  Checking for these cases reduces to the problem of finding all cycles in a graph, which promised to be very difficult to work into the follow-set generator algorithm, so instead I borrowed a trick from my undergraduate research project of building an "exclude list" of subproblems that have already been visited.

I started taking a similar functional approach to finding individual entries of the parse table, and as I worked, I found myself building in more features that were relevant to the parser itself, such as allowing semantic action symbols to appear in the grammar, and having the first and follow set and parse table generators simply ignore them when appropriate, as long as a list of them is provided.

I also noticed the inefficiency in computing the sets in this manner, that sets that were subproblems in the computation of other sets were revisited.  I decided, even though the set-computation functions were working just fine (I tested them on some of the small example grammars from Louden as I went), that it might be fun to try a bit of the memoization/dynamic programming technique using hash tables to store sets already built.

So pretty soon I ended up with the ability to feed my code any LL(1) grammar and have it compute any individual parse table entry on-the-fly, if it hadn't already computed and memoized it.  I realized gradually as the process went on that I was falling somewhat inadvertently into the design decision of having a self-generating parser.  It was less trouble to incorporate my first and follow set and parse table generators into the parser itself than to copy their output as dead data into a parser -- not to mention the added benefit of being able to test the parser on simpler grammars, and the ability to tweak the placement of semantic actions and see the effect on the parse tree right away.

It took some thinking to figure out what the best way would be to handle semantic actions and thereby build the AST.  Already my parser/generator took in a list of symbols which it should consider to be semantic actions.  The single most important purpose of  but each kind of AST node would need different logic for building itself out of elements on the semantic stack, so this sounded like a good case for OO polymorphism.  Conveniently, CLOS class names are just plain old Lisp symbols, like the symbols I used to represent both semantic actions and grammar symbols.  I decided that when a semantic action was seen on the parse stack, it should be taken as the name of a CLOS class to instantiate, call a constructor method on (whose arguments would be the consumed token and the parse stack), and then push onto the semantic stack.

I began this phase by first reconstructing the ac grammar, testing the parse algorithm on it, then adding semantic actions and abstract syntax classes.  It turned out that the semantic stack didn't work quite the way I thought it would.  Common Lisp has functions for treating lists as stacks, with side-effects on the lists, but it turned out that if I passed the semantic stack to my build function for an AST node, the side-effects of popping the stack there stayed local to that function, and did not affect the state of the stack as far as concerned the parse function from which the build function was called.  I did not expect this.  In addition, the build function did not return the abstract syntax object implicitly, as I had mistakenly thought it would -- this was an error in my thinking about the CLOS, some time having passed since I had last worked with it.  Instead it returned the value returned by the last expression in the method used, which was usually a setf of a slot on the AST object.

Rather than add an explicit return of the AST node object to each build method, I decided that each would return the semantic stack when it was done with it, and that the parse function would recurse using this new value for the semantic stack.  I'm not sure if this is the best design.  I could have probably written a side-effect-preserving stack object (even with just a small closure) that could be passed around, but it does seem like a good idea to have each AST node's build function have more control over the state that the semantic stack should be in when it finishes its work by having to explicitly return it.

In fact, this flexibility came to good use when I tried to implement the AST nodes for declarations and statements.  How to get the node to subsume the proper (arbitrary) number of items from the semantic stack at the right time eluded me; because of a misunderstanding of how the grammar was being followed (probably owing to being tired -- late-night hacking is overrated if you ask me), my original design would only consume the top declaration or statement and leave the others on the semantic stack.  I hadn't yet looked closely enough at the code of the new Java ac parser we looked over in class to recall how this was handled there, and finally I had to sleep on it to come up with an answer.  I moved the semantic actions for creating these sequence nodes to just *before* their symbols in the grammar, and had them initially construct themselves as an empty sequence.  Afterwards, any semantic action that builds a declaration should expect to find a declarations node somewhere in the semantic stack, and likewise for statements, so I had the build methods for declarations and statements pop whatever items they needed from the stack first and then, rather than merely push themselves onto the top, request the top item on the stack, expected to be a declarations or statements node at this point, to add them to itself.  By this method, I successfully parsed sample1.ac around noon Wednesday.

From then on, it seemed as if completing the parser was primarily a matter of building a grammar with semantic actions and AST classes for Klein with the benefit of what I had learned doing the same for ac.  This was mostly the case, but there turned out to be a snag in that the follow-set generator, despite having passed the various simple tests I had given it, was still somehow not working properly with Klein's grammar.  It took quite some time hacking at the grammar, parser, and AST code before I was able to determine that this was thesource of the problem by comparing its output for a relevant subset of the grammar with hand-computed follow sets different versions of the same subset from before and after applying left-factoring and left-recursion removal.  This was on Sunday, so hand-computing first and follow sets for the entire Klein grammar, even just to have available as test cases, did not seem feasible, but nonetheless I finally managed to rewrite the follow-set function correctly.  The new follow-set function is considerably more readable, and gets rid of some accidental mistaken assumptions made by the original.

After a few small bugfixes, the parser correctly parsed euclid.kl; after a bit more tinkering, it correctly parsed countchange.kl, though in the process I discovered what I perceived to be some points where the grammar was inconsistent with what understanding I thought I had of Klein; these actually caused me to actually make changes to countchange.kl so that it would parse consistently with the program logic.  I am still up in the air about operator precedence among arithmetic operators, as the language spec seems to leave this question completely open, and am not entirely sure that I have the precedence among the different categories of operators correct.  What I do know is that should any of these turn out to be occurring incorrectly, correcting them should be a simple matter of adjusting the contents of *klein-grammer-semantic*.

To see the parser in action, load up "klein-demo.lisp", change the *klein-dir* variable to you location, compile, and run the function parser-demo.  Output showing the state of the parser should appear during the parsing, followed by a print of the abstract syntax.
